{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langsmith import traceable\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import ChatWatsonx\n",
    "\n",
    "llm_params = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"min_new_tokens\": 1,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "\n",
    "llm = ChatWatsonx(\n",
    "    model_id=\"meta-llama/llama-3-1-70b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.environ[\"WX_PROJECT_ID\"],\n",
    "    apikey=os.environ[\"WX_API_KEY\"],\n",
    "    params=llm_params\n",
    ")\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- crawl a website [DONE]\n",
    "- check if it's 500 hundred words, split it if needed and save it\n",
    "- run the workflow\n",
    "    - retrieve the md using the input name\n",
    "    - generate multiple questions and answers\n",
    "    - select the best 3 \n",
    "    - save the qna json list\n",
    "\n",
    "- create the YAML\n",
    "- launch instructlab\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Parser\n",
    "class QuestionsandAnswers(BaseModel):\n",
    "    question: str = Field(description=\"question related to the context\")\n",
    "    answer: str = Field(description=\"answer to the question using the context\")\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=QuestionsandAnswers)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LLM\n",
    "template = \"\"\"\n",
    "You are a robot that only outputs JSON.\n",
    "You reply in JSON format, do not introduce your answer.\n",
    "Using the provided context, create a relevant question and answer, that would help deepen someone's knowledge.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"decoding_method\": \"sample\",\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"min_new_tokens\": 10,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1,\n",
    "}\n",
    "\n",
    "llm = WatsonxLLM(\n",
    "    model_id=\"meta-llama/llama-3-1-70b-instruct\",\n",
    "    # model_id=\"meta-llama/llama-3-405b-instruct\",\n",
    "    url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "    project_id=os.environ[\"WX_PROJECT_ID\"],\n",
    "    apikey=os.environ[\"WX_API_KEY\"],\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "qna_chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from mypy_extensions import TypedDict\n",
    "from typing import List, Annotated\n",
    "import operator\n",
    "from langgraph.types import Send\n",
    "import json\n",
    "\n",
    "# States\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        md_filename: markdown filename\n",
    "        context: question\n",
    "        questions_and_answers: LLM generation\n",
    "    \"\"\"\n",
    "\n",
    "    md_filename: str\n",
    "    iterations: list\n",
    "    context: str\n",
    "    questions_and_answers: Annotated[list, operator.add]\n",
    "    \n",
    "\n",
    "class QnAState(TypedDict):\n",
    "    context: str\n",
    "    iteration: int\n",
    "\n",
    "\n",
    "# Node function\n",
    "def retrieve_markdown(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    md_filename = state[\"md_filename\"]\n",
    "\n",
    "    # Retrieval\n",
    "    markdown_path = os.path.join(\"yaml_construction/context\", md_filename)\n",
    "    loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "    document = loader.load()\n",
    "    print(state[\"iterations\"])\n",
    "    return {\"md_filename\": md_filename, \"context\": document}\n",
    "\n",
    "\n",
    "def generate_qna(state: QnAState):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "    \"\"\"\n",
    "    context = state[\"context\"]\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    generation = qna_chain.invoke({\"context\": context})\n",
    "    return {\"questions_and_answers\": [generation]}\n",
    "\n",
    "\n",
    "def select_qna(state):\n",
    "    print(\"---SELECT---\")\n",
    "    \n",
    "    for qna in state[\"questions_and_answers\"]:\n",
    "        print(qna['question'])\n",
    "\n",
    "    # Saving\n",
    "    filename=state[\"md_filename\"]\n",
    "    qna_dict = {\"questions_and_answers\": state[\"questions_and_answers\"]}\n",
    "    fp = f\"yaml_construction/questions_and_answers/{filename}.json\"\n",
    "    with open(fp, \"w\") as f:\n",
    "        json.dump(qna_dict, f)\n",
    "\n",
    "\n",
    "# Edge functions\n",
    "def continue_to_qna(state: GraphState):\n",
    "    return [Send(\"generate_qna\", {\"context\": state[\"context\"], \"iteration\": i}) for i in state[\"iterations\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "graph = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes and edges\n",
    "graph.add_edge(START, \"retrieve_markdown\")\n",
    "graph.add_node(\"retrieve_markdown\", retrieve_markdown)  # markdown retriever\n",
    "graph.add_conditional_edges(\"retrieve_markdown\", continue_to_qna ,[\"generate_qna\"])\n",
    "graph.add_node(\"generate_qna\", generate_qna)  # generatae\n",
    "graph.add_edge(\"generate_qna\", \"select_qna\")\n",
    "graph.add_node(\"select_qna\", select_qna)  # grade the questions\n",
    "graph.add_edge(\"select_qna\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start workflow\n",
    "for markdown in os.listdir(\"yaml_construction/context\")[:]:\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"Converting {markdown}\")\n",
    "            for s in workflow.stream({\"md_filename\": markdown, \"iterations\": list(range(3))}):\n",
    "                print(s)\n",
    "\n",
    "        except Exception:\n",
    "            print(Exception)\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
