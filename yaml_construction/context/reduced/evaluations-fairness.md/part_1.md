# Fairness evaluations Last Updated: 2024-10-03 You can configure fairness evaluations to determine whether your model produces biased outcomes. Use fairness evaluations to identify when your model shows a tendency to provide favorable outcomes more often for one group over another. ## Configuring fairness evaluations for traditional machine learning models[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#configuring-fairness-evaluations-for-traditional-machine-learning-models "Copy to clipboard") If you [log payload data](/docs/en/SSQNUZ_5.0.x/wsj/model/wos-manage-payload-data.html) when you [prepare for model evaluations](/docs/en/SSQNUZ_5.0.x/wsj/model/wos-deploy-prepare.html), you can configure fairness evaluations. You can configure fairness evaluations manually or you can run a [custom notebook](https://www.ibm.com/links?url=https%3A%2F%2Fgithub.com%2FIBM%2Fwatson-openscale-samples%2Fblob%2Fmain%2Ftraining%2520statistics%2F4.6%2Ftraining_statistics_notebook.ipynb) to generate a configuration file. You can upload the configuration file to specify the settings for your evaluation. When you configure fairness evaluations manually, you can specify the reference group (value) that you expect to represent favorable outcomes. You can also select the corresponding model attributes (features) to monitor for bias (for example, Age or Sex), that will be compared against the reference group. Depending on your training data, you can also specify the minimum and maximum sample size for evaluations. ### Select favorable and unfavorable outcomes[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#select-favorable-and-unfavorable-outcomes "Copy to clipboard") You must specify favorable and unfavorable outcomes when configure fairness evaluations. The values that represent a favorable outcome are derived from the `label` column in the [training data](/docs/en/SSQNUZ_5.0.x/wsj/model/wos-manage-training-data.html). By default the `predictedLabel` column is set as the `prediction` column. Favorable and unfavorable values must be specified by using the value of the `prediction` column as a string data type, such as `0` or `1` when you are uploading training data. ### Select features[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#select-features "Copy to clipboard") You must select the features that are the model attributes that you want to evaluate to detect bias. For example, you can evaluate features such as `Sex` or `Age` for bias. Only features that are of categorical, numeric (integer), float, or double fairness data type are supported. The values of the features are specified as either a reference or monitored group. The monitored group represents the values that are most at risk for biased outcomes. For example, for the **`Sex`** feature, you can set `Female` and `Non-binary` as the monitored groups. For a numeric feature, such as **`Age`** , you can set `[18-25]` as the monitored group. All other values for the feature are then considered as the reference group, for example, `Sex=Male` or `Age=[26,100]`. ### Set fairness threshold[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#set-fairness-threshold "Copy to clipboard") You can set the fairness threshold to specify an acceptable difference between the percentage of favorable outcomes for the monitored group and the percentage of favorable outcomes for the reference group. For example, if the percentage of favorable outcomes for a group in your model is 70% and the fairness threshold is set to 80%, then the fairness monitor detects bias in your model. ### Set sample size[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#set-sample-size "Copy to clipboard") Sample sizes are used to spedicy how to process the number of transactions that are evaluated. You must set a minimum sample size to indicate the lowest number of transactions that you want to evaluate. You can also set a maximum sample size to indicate the maximum number of