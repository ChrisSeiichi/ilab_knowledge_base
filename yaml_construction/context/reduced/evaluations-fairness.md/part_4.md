formula calculates the impact score: selection rate for monitored groups Impact score = ________________________________________________________ selection rate for reference groups Copy to clipboard * **Thresholds** : * Lower bound: 0.8 * Upper bound: 1.0 * **How it works** : Higher scores indicate higher selection rates for monitored groups ## False negative rate difference[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#false-negative-rate-difference "Copy to clipboard") The false negative rate difference gives the percentage of positive transactions that were incorrectly scored as _negative_ by your model. * **Description** : Returns the difference in false negative rates for the monitored and reference groups * **At 0** : Both groups have equal benefit. * **Uses the confusion matrix to measure performance** : Yes * **Do the math** : The following formula is used for calculating false negative rate (FNR): false negatives False negative rate = __________________________ all positives Copy to clipboard The following formula is used for calculating false negative rate difference: False negative rate difference = FNR of monitored group - FNR of reference group Copy to clipboard ## False positive rate difference[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#false-positive-rate-difference "Copy to clipboard") The false positive rate difference gives the percentage of negative transactions that were incorrectly scored as _positive_ by your model. * **Description** : Returns the ratio of false positive rate for the monitored group and reference groups. * **At 0** : Both groups have equal odds. * **Uses the confusion matrix to measure performance** : Yes * **Do the math** : The following formula is used for calculating false positive rate (FPR): false positives False positive rate = ________________________ total negatives Copy to clipboard The following formula is used for calculating false positive rate difference: False positive rate difference = FPR of monitored group - FPR of reference group Copy to clipboard ## False discovery rate difference[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#false-discovery-rate-difference "Copy to clipboard") The false discovery rate difference gives the amount of false positive transactions as a percentage of all transactions with a positive outcome. It describes the pervasiveness of false positives among all positive transactions. * **Description** : Returns the difference in false discovery rate for the monitored and reference groups. * **At 0** : Both groups have equal odds. * **Uses the confusion matrix to measure performance** : Yes * **Do the math** : The following formula is used for calculating the false discovery rate (FDR): false positives False discovery rate = _________________________________________ true positives + false positives Copy to clipboard The following formula is used for calculating the false discovery rate difference: False discovery rate difference = FDR of monitored group - FDR of reference group Copy to clipboard ## False omission rate difference[](/docs/en/cloud-paks/cp-data/5.0.x?topic=evaluations-fairness#false-omission-rate-difference "Copy to clipboard") The false omission rate difference gives the number of false negative transactions as a percentage of all transactions with a negative outcome. It describes the pervasiveness of false negatives among all negative transactions. * **Description** : Returns the difference in false omission rate for the monitored and reference groups * **At 0** : Both groups have equal odds. * **Uses the confusion matrix to measure performance** : Yes *