# Governing AI Model Lifecycle with IBM Watson OpenScale and IBM Open Pages

One of the big challenges in enterprise adoption of AI is the lack of tools and techniques to track and govern the entire end to end AI model lifecycle. AI Governance is a pretty broad area which covers multiple things such as data and model lineage, policies around model quality and its usage, model version management and tracking, etc. One of the critical things for AI Governance is that of tracking the end to end lifecycle of AI models and storing the associated metadata. In this blog we outline how this can be achieved using IBM Open Pages and IBM Watson OpenScale.

## AI Model Lifecycle

Model Creation Request: The first step is that of generating the model creation request. This outlines the business need for building the model, its constraints, likely usage, downstream impact, etc.
Model Development: The next step is that of the data scientist building the model.
Model Validation: Once the model is built, it is validated by a team of model validators.
Model Approval: The validation report generated by the model validators is then analysed by model approvers who decide if the model should be approved or sent back to the data scientist for additional changes.
Model Deployment: Once the model has been approved, it is deployed into the production environment.
Model Monitoring: The production model needs to be continuously monitored for things such as fairness, drift, quality, etc. If any issues are detected in the model, a new version of the model is created and the entire validation/approval process is repeated on the new model version.
Model Retirement: If the model is no longer required, it is finally retired.
IBM Open Pages provides capability to define custom workflows which can be used to track and govern the different stages of the model lifecycle. Using Open Pages users can create a model creation request. Once the request is created, based on the defined workflow, it will notify the specified data scientists about the request so that they can start building the model. Once the model is built, the data scientist can document the model metadata in Open Pages and mark the model creation step as complete. Open Pages will then notify the model validators to validate the model.

One of the challenges faced by model validators especially in the banking industry is that although they are well conversant with validating statistical models, they do not have the skills needed to validate AI models. This is where IBM Watson OpenScale helps. As mentioned in our previous blog post, OpenScale allows model validators to validate AI models without writing a single line of code! On top of that, OpenScale has deep integration with Open Pages. Hence we can link a model in OpenScale with its counterpart in Open Pages (see Figure 2). Once this link is established, the model validator can push the metrics computed using OpenScale to Open Pages. Additionally the model validator can also push the model validation report generated using OpenScale to Open Pages. Thus these capabilities allow model validators to easily validate an AI model and also push the results of the validation to Open Pages.

Once the model validation is finished, the model validator marks the step as complete in Open Pages. Open Pages then notifies the model approvers to approve/reject the model. The model approver will look at the metrics computed by OpenScale to decide if the model needs to be approved or rejected. As the metrics are available within Open Pages (See Figure 3), the approvers can do their entire work in Open Pages.

After the model has been approved, it is deployed to production. OpenScale again plays a critical role to continuously monitor the model in production. It not only generates alerts whenever any of the metrics exceed their thresholds but also pushes these threshold violations data to Open Pages. One can define workflows in Open Pages to act on these threshold violations such as starting a new workflow which will notify the data scientist to re-evaluate the model and/or build a new version of the model.

Thus IBM Watson OpenScale and Open Pages have a deep integration and provide a comprehensive capability to track and govern the end to end AI Model lifecycle.

# AI Model Validation without writing a single line of code!

Model development using automated tools is a well-known concept. But is it really possible to validate an AI model without writing a single line of code? The answer is Yes! IBM Watson OpenScale provides Model Risk Management capability which allows model validators to validate AI models without writing any code. In this blog post, we provide an overview of this capability and how enterprises can leverage this to speed up their adoption of AI.

## Challenges with AI Model Validation

Ask any software exec what processes they have in place before any software is deployed to their production environment and they would tell you about their time tested and robust methodology. If you were to flip the question and ask what process they employ for validating AI models before they are deployed to production, chances are that the answer will be far less convincing.

There are many reasons for this. One of the big challenges in AI Model validation is that just like model development, model validation is complex. It involves multiple things such as checking for bias, understanding model quality (precision, recall, false positive rate, RMSE, etc.), understanding model drift, comparing with a challenger model, etc. Such validation requires specialised resources who can understand and apply these concepts — resources which, unfortunately, are hard to find.

The second problem with AI model validation is the lack of a standardised way to validate and document the results of model validation. Different validators might use different techniques to validate and document the results of model validation. This leads to a lack of uniformity across the enterprise.

## Model Risk Management in OpenScale

If a model validator has to validate an AI model, he/she needs to understand how to score the model, how to understand and interpret the output to figure out if the model is exhibiting bias or drift and find out about the model quality. The Model Risk Management capability in OpenScale allows model validator to validate an AI model by simply uploading some test data. OpenScale takes care of performing different kinds of tests for bias, quality, drift as well as model explainability. Thus the model validator can validate the AI model without writing a single line of code!

Let us now get into the details of how things work in Model Risk Management. AI model validation is typically done in a pre-production environment. One of the first capabilities that OpenScale provides is a way to tag a model serve environment as either production or pre-production.

When an environment is tagged as a pre-production environment OpenScale will assume that models deployed in this environment would be validated by a model validator. For such models, OpenScale provides an easy to use GUI to validate these models. Once the deployment has been configured, the model validator can simply upload some test data. OpenScale will then validate the model along different dimensions such as fairness, drift, quality as well as explainability. The model validator is provided a summarised dashboard (see Figure 2) which provides an easy to understand summary of the tests that were run and how many of them passed.

The validator can also choose to download the results as a PDF document so that they have a record of all the tests that were done and its results.

One very important capability that is provided by OpenScale is that of comparing the model under validation with a challenger model (see Figure 3). The challenger model could be a model which is already deployed in production or could be a model which the model validator builds using tools such as Auto AI. This provides confidence to the model validator that the model under validation is good/better than the one deployed in production.

Thus the use of Model Risk Management allows model validators to validate AI models without writing a single line of code. It also brings in uniformity across the validations done by all the validators thereby ensuring that the right standards are followed while validating AI models. Finally, the documentation capability of OpenScale helps validators to store the results of their validation.

# Counterfactual Fairness and IBM Watson OpenScale

There are multiple metrics and associated techniques to measure fairness in AI Models. One such metric is Counterfactual fairness. In this blog post, we explain how the fairness detection technique in IBM Watson OpenScale also helps to detect counterfactual fairness in AI Models.

## Issue with Fairness Metrics

Consider a scenario where a bank is using an AI model to decide if a person who is applying for a loan should be grated the loan or not. The bank would like to ensure that its AI model acts in a fair manner against people of all ethnicities, genders and age groups. Metrics such as Disparate impact ratio can be used for this purpose. Disparate impact ratio compares the percentage of loan approved outcomes for (say) different age groups and signals a bias if there is a big difference for different age groups. E.g., if people in the age group [26,90] get a loan approved outcome 60% of the times whereas people in the age group [18,25] get a loan approved outcome only 20% of the times, then such a model will be flagged as acting in a biased manner.

However, there is a problem with the above approach. Consider a scenario where all the people in the age group [18,25] have a poor credit rating. The AI model is then bound to give a loan denied outcome to such applicants. Hence the loan approved outcome for people in the age group [18,25] will be 0% and the model will be said to act in a highly biased manner as per Disparate Impact ratio. However in reality, the model is making the right decision and is not acting in a biased manner. This problem is addressed by Counterfactual fairness.

## Counterfactual fairness

The AI model mentioned earlier is said to be Counterfactually fair if it gives the same prediction had the person had a different race/gender or age group. Many a times model developers do not make use of attributes such as race/gender when building an AI model. So if such attributes are not used as features of the model, how do we figure out the model behavior when the gender/race is changed? Counterfactual fairness addressed this problem as follows:

- Step 1: The first thing needed for computing Counterfactual fairness is to identify relationships between fairness attributes (such as race, gender, age, etc.) and one or more features of the model. E.g., Race is correlated with Zip code and Income. This is a manual step and needs to be done by domain experts.

- Step 2: Consider that an African American person applied for a loan and the model predicted that the Loan should be denied. In order to find Counterfactual fairness, we assume that this person was from a different race.

- Step 3: We then find the values of all the correlated features which have a correlation with race. This way we will find the new values of these features when the race is changed from African American to say Caucasian. This new record is sent to the model and if the model prediction changes to say Loan approved, then the model is said to be Counterfactually unfair.

There are two key things which we would like to highlight here: (1) There is a manual effort involved in finding relationships between fairness attributes and one or more features (in Step 1 above) and (2) It computes individual bias. In other words, if the model exhibits bias for a single person, then it is said to be biased.

## Fairness in Watson OpenScale

IBM Watson OpenScale makes use of a data perturbation based fairness detection technique which is similar to what is done in Step 2 and 3 of Counterfactual fairness method mentioned above. The details of these techniques are listed in our earlier blogs on fairness and indirect bias. There are two major improvements that OpenScale supports:

- OpenScale helps to automatically identify the relationships between the fairness attributes and one or more features of the model. This avoids the need to do this manually which is error prone.

- OpenScale makes use of a technique which is a hybrid of individual + group fairness. The details of this technique are given below.

## Group + Individual Fairness

Consider a scenario where an AI model has made predictions on 100 records. In the hybrid technique, OpenScale first computes individual fairness by perturbing multiple records (from the 100 records) sent to the model. In other words if an African American person has applied for a loan, OpenScale will flip the race to say Caucasian and also change the correlated features (such as zip code, income level, etc.). This perturbed record is sent to the model and the model output is stored. Such perturbation is done for multiple records.

In the second step, we compute the group fairness, over the combination of the original records received by the model (100 in the above example) and the perturbed records generated in the previous step (which could be close to 100 or more). This step helps us understand the overall behavior of the model as opposed to flagging fairness even if a single record is not counterfactually fair. This is especially useful in an enterprise setting where it is difficult to build a model which is 100% counterfactually fair and what they really need is a way to understand the overall model behavior. Having said that, if an enterprise wants to ensure that their AI model is fair on each and every record (in other words the model is counterfactually fair), OpenScale supports that as well.

Thus the fairness detection technique in IBM Watson OpenScale builds on top of Counterfactual fairness and is especially suited for use by enterprises.

# Indirect Bias support in IBM Watson OpenScale

Fixing Bias is very easy — all I need to do is make sure that the model does not make use of features such as Gender, Ethnicity, etc. If the model is not aware of Gender, Ethnicity, etc., then it cannot be biased on these attributes.

At first glance the above argument looks reasonable — however, nothing could be farther from truth. Unfortunately a lot of enterprises have learnt this the hard way. If you look at any of the recent issues related to bias in AI models, you will find that most of the models did not have ethnicity or gender as a feature but still they ended up exhibiting bias! This is because there could be other features used by the model which have a correlation with these attributes. E.g., Zip code and Income can have a strong correlation with Ethnicity. This might sound obvious, but correlation can creep up in unexpected and interesting ways some of which are hard to detect. One such example is that of the feature “Has_car” which signifies whether the person has a car or not. This is correlated to Ethnicity.

## Indirect Bias Problem

The standard fairness metrics assume that the fairness attributes such as Ethnicity, Gender, Age, etc., is used as one of the features of the model. However, many enterprises have an internal rule that mandates that such attributes should not be used as features of a model. This brings in an interesting problem — how will we measure fairness if the fairness attribute is not one of the features of the model? Detecting bias on attributes which are not used as features of the model is called as Indirect Bias. In this blog post we provide an outline of how we support Indirect Bias detection capability in OpenScale. Detection of Indirect bias is a two-step process:

- Correlation Identification

- Indirect Bias detection

Correlation Identification: In the first step the user needs to provide OpenScale the training data used to build the model. This data needs to be augmented with the fairness attribute value as well. E.g., Consider a scenario where a user is building a credit risk model to predict if a loan application is risky or not. When building the model, the data scientist makes use of training data which does not include the Gender of the person. In order for OpenScale to detect Indirect Bias, what the data scientist needs to do is to find the Gender for each record in the training data and add that as an additional column to the training data. This augmented data needs to be provided to OpenScale. Please note that the model will still be built without using the Gender — it’s just that OpenScale needs information about the Gender for each record in the training data. Once this information is made available to OpenScale, it automatically finds correlation between the Gender and one or more features of the model.

Indirect Bias Detection: The second step is that of the actual indirect bias computation. As one would recollect, OpenScale needs access to the payload data which is the input and output of the model at runtime. If we consider the example of the credit risk model mentioned earlier, it does not make use of Gender as one of the features of the model and hence the Gender value will not be part of the model payload. However, in order for OpenScale to detect Indirect Bias, the user needs to send this additional Gender column along with the payload data. Thus OpenScale now has information about the Gender for each record which has been scored by the model (even though the model did not make use of the Gender when making the prediction). In addition to this, OpenScale also has information about the correlation that it has found between the Gender and one or more features of the model. Once we have the above information, we make use of this data to find the fairness of the model using the same perturbation based technique outlined in our earlier blog post.

In summary, OpenScale uses an innovative correlation identification based technique to monitor and detect bias for models which do not make use of the fairness attribute as one of the features of the model. This technique is better than Counterfactual fairness as OpenScale automatically finds the correlation between the features —something that needs to be done manually by a domain expert in Counterfactual Fairness based approach.

# How to implement fair and unbiased technology with IBM Watson OpenScale

The movement to eliminate bias touches every corner of society, and information technology — especially artificial intelligence — is no exception. In his June 8, 2020 Letter to Congress on Racial Justice Reform, IBM CEO® Arvind Krishna reaffirmed IBM’s commitment to advance racial equality, asserting the company’s historic commitments to equal opportunity and justice. He quoted the 1953 statement of IBM President Thomas J. Watson, who said:

“. . . Each of the citizens of this country has an equal right to live and work in America. It is the policy of this organization to hire people who have the personality, talent and background necessary to fill a given job, regardless of race, color or creed.”

Of the three recommendations Arvind Krishna offered to Congress — police reform, responsible technology policies, and expanding opportunity — IBM has the power to influence two directly. From the design of products and services, to the development and validation of best practices for their implementation and use, IBM is committed to help clients ensure that that their technology and offerings support the pursuit of justice and racial equity. This blog outlines how IBM helps organizations achieve these goals using IBM Watson OpenScale.

## Using AI to prevent bias in hiring
AI models may be used to improve equity in the hiring process. A retail client using model monitoring employed an AI model to create a short list of eligible candidates for interviews. Retail companies receive thousands of applications and AI models can help narrow applicant lists to identify the best candidates for each position. It is critical that AI models not perpetuate bias in hiring. How can bias compromise fairness in hiring? A model trained to prefer a candidate who “has her own transport,” may inadvertently eliminate a qualified person of color. Why? The National Equity Atlas reported in 2015 that nearly 15 percent of people of color neither own nor have access to a car. This rate is more than double that of whites, at 6.5 percent. The model that privileges car ownership may bias hiring away from people of color. To detect and avoid bias, the client used IBM Watson OpenScale not only to ensure that the model was bias-free at the time of design but also that it continued to remain bias-free at runtime. Read more about how explainable AI helps enterprises monitor models for biased decision-making.

## Using AI to prevent bias in training
Training and education for in-demand skills is key to expanding economic opportunity for disadvantaged communities. And AI can help expand opportunity by playing a more constructive role in employee training. For instance, an IBM client uses an AI model to recommend a set of trainings for in-demand, high-paid skills in cloud or cybersecurity. Historically, these trainings would have been offered only to employees with advanced degrees. But with the advent of specialized training for “new collar” jobs, many employees now possess the necessary skills once achievable exclusively through four-year college degrees. When models exhibit preferences toward employees with college or graduate-school degrees, they may inadvertently express bias toward groups of people whose qualifications may not fit a stereotypical educational model.

## Using AI to audit and report model behavior
Whenever AI models are used, businesses must maintain a historical record of decisions based on those models. This is especially critical for regulated industries like banking. Governance best practice includes maintaining an audit trail of AI model bias, model quality and model drift. IBM Watson OpenScale helps address these requirements by keeping a historical record of all model input, output and documentation of all model metrics, ensuring that clients can explain the behavior of the AI model and prove to regulators that the AI models they use have not shown bias in the past.

Fairness in hiring and training is not limited to human behavior. Technology can play an equally important role in promoting fairness and preventing bias, especially with AI, which begins with monitoring and ensuring AI models behave properly. Using IBM Watson OpenScale for model monitoring helps enterprises catch hidden biases in AI models before they become a problem, a must-have for all enterprises employing AI models in decisions regarding human performance.

# Understanding Model Drift with IBM Watson OpenScale

Airline pilots have the big responsibility of flying passengers safely from one place to another. In order to ensure that they are fit to discharge their duties, they have to undergo multiple tests at periodic intervals. E.g., medical assessment to ensure they are fit to fly, Line Check to test their flying skills, simulator practice for emergency procedures, etc. A similar need exists for AI Models. With the widespread adoption of AI in the enterprise, AI models carry the responsibility of ensuring correct business decisions. E.g., a loan processing model makes a decision on who should get a loan whereas a marketing campaign model decides who should be targeted for a marketing campaign. Before these models are deployed to production, enterprises ensure that they have good accuracy and perform well on the test data.

However, as with pilots, it is important to perform periodic checks on AI models to ensure that they are continuing to perform at the expected level. One way to do that is to generate new manually labelled data at periodic intervals and use that to test the model accuracy. However, generating manually labelled data is an expensive and time consuming process. Hence it is difficult to generate such data and even if such data were to be available, the amount of manually labelled data is always limited and does not cover all kinds of data which the model receives in production. Therefore, testing accuracy using limited manually labelled data does not guarantee that the model is performing accurately on the diverse data received by the model in production.

In order to address the above problem, IBM Watson OpenScale has introduced a new feature for Drift Detection. It helps enterprises to (a) monitor the accuracy of their models in production without having to generate manually labelled data and (b) identify a change in the characteristics of the data received by the model at runtime as compared to the training data. Poor model accuracy can often be attributed to model input that differs from the original training data or input which causes the model accuracy to drop. The Drift Detection capability in IBM Watson OpenScale monitors the data received by the model in production and (a) estimates the accuracy of the model (accuracy drift) and (b) checks if the data is very different than the models’ training data (data drift). Accuracy Drift is very helpful to enterprises as it helps them to immediately react to a drop in model accuracy before it has any significant impact on the business outcomes. E.g., In case of the loan processing model it will ensure that the business does not end up giving loans to a wrong set of customers thereby avoiding bad loans. Data drift on the other hand helps enterprises understand the change in data characteristics at runtime which might point to a change in the business environment. E.g., In case of the loan processing model the training data might have had very few applicants with age < 25. Due to a marketing campaign run to attract younger customers, a lot of young people might be applying for a loan. This will trigger a data drift alert which will point to the fact that the model might not be trained to handled such kind of customers. Hence the business would want to retrain the model to ensure that it makes the right decision for people with age < 25.

## Configuring Drift Detection in OpenScale
When the customer configures Drift in OpenScale, they have to specify the tolerable accuracy drift magnitude. The drift is measured as the drop in accuracy as compared to the model accuracy at training time. E.g., if the model accuracy at training time was 90% and at runtime the estimated accuracy of the model is 80%, then the model is said to have drifted by 10%. Depending on the use case, model owners will be willing to tolerate different amounts of drift. Hence IBM Watson OpenScale allows the user to specify the accuracy drift magnitude (called as Drift alert threshold) for each model being monitored in OpenScale. If the drift for a model drops below the specified threshold, then OpenScale will generate an alert for the user.

## How does Drift detection work?
In order to identify the accuracy drift, OpenScale needs to understand the behaviour of the customers’ model on the training and test data. It analyses the customer model behaviour and builds its own model (called Drift Detection Model) which predicts if the customers’ model is going to generate an accurate prediction for a given data point. OpenScale runs the Drift detection model on the payload data (data received by the model at runtime). Thus we now know the number of records in the payload for which the customers’ model is likely to have made an error in prediction. We make use of this information to generate the predicted accuracy of the customer’s model on the payload data. This value is compared with the accuracy of the model at training time to identify the model accuracy drift.

IBM Watson OpenScale identifies data drift by analysing the training data and extracting some characteristics of the data. It then compares the data received by the model at periodic intervals with the training data characteristics to identify data drift.

## Reporting Drift

The Drift detection GUI is shown in Figure 1. It has two curves: the blue line represents the drop in model accuracy as compared to training time. The green line represents the percentage of data which has data drift. In the above Figure, the accuracy drift (at the extreme right point) is 20% which is 10% below the threshold. The model accuracy drift is shown as a range in the GUI with a thick center line. The center line represents the actual predicted accuracy. However, finding the exact predicted accuracy is a very hard problem. Hence the GUI also reports a range in which the drift is likely to lie. The model is said to be drifted if the predicted accuracy (center of the band) crosses the threshold. The data drift in the above figure is reported as 10%. This implies that 10% of the data received by the model is different than the models training data.

Once the drift has been identified, the user can drill down at a specific point in time to understand the reasons for the accuracy and data drift as shown in Figure 2. In the above example, OpenScale is showing three groups of transactions which have drifted using a Venn diagram. The first group represents the transactions which have contributed to the accuracy drift, the second group represents transactions which have data drift and the third group has transactions which have both accuracy and data drift. Clicking on one of the group allows the user to understand a summary of the transactions belonging to that group.

These transactions are grouped based on the features which played a key role in the drift. E.g., the first box has two features Profession and State. This signifies that the values of these two features played a key role in OpenScale believing that the model output is incorrect. Please note that each box represents multiple transactions received by the model. The user can click on each box to see the list of transactions represented by that box.

Customers can make use of these transactions and send them for manual labelling. This manually labelled data can then be used to retrain the model so that the model accuracy does not drop at runtime. Thus IBM Watson OpenScale not only helps to identify the drift but also helps the user to understand the root cause of the drift and provides drifted data which can be used to fix the model drift.

## Summary
Thus in summary, model accuracy and data monitoring are very critical to ensure that the model is continuing to perform at the expected levels once it is deployed to production. The Accuracy Drift detection capability in IBM Watson OpenScale helps enterprises monitor their models’ accuracy at runtime without having to generate manually labelled data. On the other hand, Data Drift detection capability helps enterprises understand the change in data characteristics at runtime. Thus the accuracy and data drift ensures that enterprises can immediately react to a change in model accuracy and business environment before it has any significant impact on the business outcomes.

# De-Biasing in IBM Watson OpenScale

Can AI be used to help improve AI? With the recent advances in AI this is increasingly becoming true. One such example is the De-biasing capability in IBM Watson OpenScale. This capability uses AI to identify whether a machine learning model is likely to act in a biased manner and then goes on to fix the bias for such models. Thus IBM Watson OpenScale not only helps customers identify Fairness issues in the model at runtime, it also helps to automatically de-bias the models. In this post, we explain the details of how de-biasing works in IBM Watson OpenScale.

Let us first start off with the advantages of the de-biasing capability in IBM Watson OpenScale:

Techniques available in open source fix the bias by randomly changing the prediction of the model. E.g., if a model which decides loan applications is seen to be acting in a biased manner against Females, then the open source techniques will randomly change the prediction of the model for some female applicant from say Loan Denied to Loan Approved. Enterprises would not be okay with such an approach as they might end up giving a loan to a customer with say a bad credit rating. IBM Watson OpenScale will never make such arbitrary decisions. It uses a new innovative approach in which all de-biasing decisions are made using the customers model — Always! Thus we use the customers biased model to make de-biased predictions. Sounds counterintuitive but is very elegant (as we explain below).
The de-biasing capability in IBM Watson OpenScale is enterprise grade. It is robust, scalable and can handle a wide variety of models. De-biasing using open source techniques requires a lot of effort to get it production ready and it might end up making incorrect business decisions.
Thus there are challenges with using open source de-biasing techniques which are addressed in OpenScale. De-biasing in OpenScale consists of a two step process:

Learning Phase: Learning customer model behaviour to understand when it acts in a biased manner.
Application Phase: Identifying whether the customer’s model will act in a biased manner on a given data point and if needed, fixing the bias.

## Learning Phase

In the learning phase, IBM Watson OpenScale looks at the predictions done by the model and identifies if the model is acting in a biased manner. E.g., if for a Female customer the model predicts “Loan Denied”, we flip the gender to Male keeping all the other feature values same and send it back to the model. If the model predicts “Loan Approved”, then we know that the customer model is acting in a biased manner against Females. We identify such data points where the customer model acts in a biased manner. Using this information we build another AI model (called bias prediction model) which takes in input a data point and is able to predict whether the customer’s model is going to act in a biased manner or not on that data point. E.g., if we have learned that whenever Zip Code=90200 and Credit Score=Medium, the client model acts in a biased manner against Females. In this case the bias prediction model will check if for the input data point, the Zip code is 90200 and the credit rating is Medium, then it will predict that the customer’s model will act in a biased manner. For all other data points, the Bias prediction model will predict that the customer’s model will not act in a biased manner.

## Application Phase

As new data is sent to the client model for scoring, it gets stored in the payload logging table by IBM Watson OpenScale. During the application phase, we send the data present in the payload table to the Bias Prediction Model. If the bias prediction model suggests that the customer model is likely to act in a biased manner on that data point, then we flip the fairness attribute from minority to majority and send it back to the client model. E.g., In the above example the fairness attribute will be Gender and if for a given transaction the value of Gender is Female and the bias prediction model predicts that the customer model is likely to act in a biased manner, then we will flip the Gender value to Male and send the modified transaction it to the customer model for prediction. The prediction of the customer model is returned as the de-biased prediction. Thus what we are doing is that for those data points where the customer model is likely to act in a biased manner, we de-bias it by returning a prediction which is same as what the customer model predicts for a majority value of the fairness attribute and all other features being the same.

The method mentioned above does de-biasing after the payload data has been logged. This is called as Passive Debiasing where the application making use of the model is not impacted. IBM Watson OpenScale also supports Active De-biasing where we provide a REST end point. This REST end point will have to be used by the application for scoring the customer model. It will internally do debiasing using the process mentioned above. If the bias prediction model suggests that the customer model is not going to act in a biased manner, then it simply returns the output of the customer model as the debiased output. Thus with Active de-biasing, the application will get de-biased output on the fly. Passive de-biasing is a very good capability for customers to gain confidence in the de-biasing capabilities and then switch to using active de-biasing.

Passive de-biasing is always performed on the payload data. The results of passive de-biasing are shown in the OpenScale GUI in the De-biased model tab (screenshot given below).

## De-biasing GUI
The GUI shows that the fairness improved from 74% to 94% due to de-biasing and it did not have any significant impact on the accuracy.

Hence in a nutshell, IBM Watson OpenScale does not arbitrarily change the prediction of the model, but boasts of an innovative technique which uses the biased customer model to come up with a de-biased prediction. It also makes use of the bias prediction model to decide which data points to de-bias. Hence AI is used to help fix problems with AI. The de-biasing algorithm mentioned in this post has been published in a conference and is available here: Bias Mitigation Post-processing for Individual and Group Fairness.

# Bias Detection in IBM Watson OpenScale

Fairness or Bias is one of the big issues that plagues the adoption of AI in enterprises. Consider the case of an insurance company which is planning to use an AI model to make a decision on whether an insurance claim should be approved or denied. LOB owners need to be absolutely sure that the model will not make biased decisions. Even if a model is known to act in an unbiased manner during training, it does not necessarily mean that it will continue to exhibit similar behaviour once it is deployed to production. IBM Watson OpenScale helps address this problem by monitoring the behaviour of AI Models in production and checking if they are acting in a biased manner.

## What is Bias?
Before we understand what is meant by bias, let us clarify a few key concepts.

Fairness Attribute: Bias or Fairness is typically measured using some Fairness attribute such as Gender, Ethnicity, Age, etc. It can also be measured using non-societal attributes such as Policy Age (to ensure that the model is not biased against new customers), etc.

Monitored/Reference Group: Monitored group are those values of Fairness attribute for which we want to measure bias. The rest of the values of the fairness attributes are called as reference group. In case of Fairness Attribute=Gender, if we are trying to measure bias against females, then Monitored group is “Female” and Reference group is “Male”.

Favourable/Unfavourable outcome: An important concept in bias detection is that of favourable and unfavourable outcome of the model. E.g., Claim approved can be considered as a favourable outcome and Claim denied can be considered as an unfavourable outcome.

Disparate Impact: This is used to measure bias and is computed as the ratio of percentage of favourable outcome for the monitored group to the percentage of favourable outcome for the reference group. Bias is said to exist if the disparate impact value is below some threshold.

E.g., if 80% of claims made by males are approved whereas only 60% of claims made by females are approved, then the disparate impact will be: 60/80 = 0.75. Typically the threshold value for bias is 0.8 and as the disparate impact ratio is less than 0.8, the model is said to be biased.

Bias Threshold: If the bias threshold is 1, then it would mean that we expect Females to get the same or better claim approval rates as compared to males. However, in some scenarios we might be okay if the monitored group gets a little bit lesser favourable outcome as compared to the reference group. In order to handle this, the customer can set the threshold value for each model to a value less than 1. Typically the value is set to 0.8.

Bias Detection in Watson OpenScale
IBM Watson OpenScale helps in detection of Bias at run time. It monitors the data which has been sent to the model as well as the model prediction (this data is called as Payload data). It then identifies bias issues and notifies them to the user. The dashboard of bias looks like follows:


Bias Detection in Watson OpenScale
The fairness attribute in the above example is Age and it shows that the model is acting in a biased manner against people in the age group 18–24 (monitored group). The percentage of the favourable outcome for people in age 18–24 is 52% whereas the percentage of favourable outcome for people in the age 25–75 (reference group) is 70%. Thus the disparate impact ratio is 0.52/0.7=0.74 which is below the threshold of 0.8 and hence the model is flagged as acting in a biased manner.

## Data Perturbation

Consider a scenario where the payload data consists of 100 insurance claims (i.e., the model has been used to make decisions for 100 claims). In these claims, lets say 60 claims were made by people in the age group 18–24 and 40 claims were made by people in the age group > 24. All the people in the age group 18–24 had very high claim frequency and were know to have made fraudulent claims in the past. Hence the model rejected all the 60 claims made by customers belonging to the age group 18–24. On the other hand, the 40 claims made by people with age > 24 were all genuine claims and hence were approved by the model. If we measure the disparate impact for this it would be 0/1 = 0. Hence we would flag the model behaviour as being biased. However, the model is not really acting in a biased manner as it is making an accurate decision based on the data provided to it. Thus if we report such a model as being biased, it is not something that the business owner would want to fix as they would obviously not want to approve a fraudulent claim just be fair.

In order to address this issue, we do data perturbation while computing fairness. The way data perturbation works is that if there is a claim by a person with say age 20 and it was rejected by the model, we perturb the record by flipping the age from 20 to a random value in the reference group (say 40) and then send this perturbed record to the model. All the other features of the record are kept the same. Now if the model predicts that the claim is approved, then it means that it is acting in a biased manner by making decisions just based on the age of the customer. Thus while computing bias, we do this data perturbation and compute the disparate impact using the payload + the perturbed data. This will ensure that we report bias only if there are sufficient number of data points where the model has exhibited biased behaviour across the original and the perturbed data. Thus we are able to detect if there is genuine bias in the model and not get impacted by the kind of data being received by the model.

In the screenshot mentioned above, the bar chart shows the statistics across the payload + the perturbed data. The percentage of favourable outcome (52%, 70%, etc.) is also computed using combination of payload + perturbed data.

## Summary

IBM Watson OpenScale provides a mechanism for enterprises to monitor if their models are acting in a biased manner at runtime. It computes bias in such a manner that it will not lead to false positives. In other words, if IBM Watson OpenScale reports a bias, it will be something that enterprises would want to fix and it will be practical for them to do so. This is a key ingredient to help build trust in AI.